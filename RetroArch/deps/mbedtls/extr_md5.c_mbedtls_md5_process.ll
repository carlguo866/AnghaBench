; ModuleID = '/home/carl/AnghaBench/RetroArch/deps/mbedtls/extr_md5.c_mbedtls_md5_process.c'
source_filename = "/home/carl/AnghaBench/RetroArch/deps/mbedtls/extr_md5.c_mbedtls_md5_process.c"
target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

%struct.TYPE_3__ = type { i64* }

; Function Attrs: noinline nounwind optnone uwtable
define dso_local void @mbedtls_md5_process(%struct.TYPE_3__* %0, i8* %1) #0 {
  %3 = alloca %struct.TYPE_3__*, align 8
  %4 = alloca i8*, align 8
  %5 = alloca [16 x i64], align 16
  %6 = alloca i64, align 8
  %7 = alloca i64, align 8
  %8 = alloca i64, align 8
  %9 = alloca i64, align 8
  store %struct.TYPE_3__* %0, %struct.TYPE_3__** %3, align 8
  store i8* %1, i8** %4, align 8
  %10 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 0
  %11 = load i64, i64* %10, align 16
  %12 = load i8*, i8** %4, align 8
  %13 = call i32 @GET_UINT32_LE(i64 %11, i8* %12, i32 0)
  %14 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 1
  %15 = load i64, i64* %14, align 8
  %16 = load i8*, i8** %4, align 8
  %17 = call i32 @GET_UINT32_LE(i64 %15, i8* %16, i32 4)
  %18 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 2
  %19 = load i64, i64* %18, align 16
  %20 = load i8*, i8** %4, align 8
  %21 = call i32 @GET_UINT32_LE(i64 %19, i8* %20, i32 8)
  %22 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 3
  %23 = load i64, i64* %22, align 8
  %24 = load i8*, i8** %4, align 8
  %25 = call i32 @GET_UINT32_LE(i64 %23, i8* %24, i32 12)
  %26 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 4
  %27 = load i64, i64* %26, align 16
  %28 = load i8*, i8** %4, align 8
  %29 = call i32 @GET_UINT32_LE(i64 %27, i8* %28, i32 16)
  %30 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 5
  %31 = load i64, i64* %30, align 8
  %32 = load i8*, i8** %4, align 8
  %33 = call i32 @GET_UINT32_LE(i64 %31, i8* %32, i32 20)
  %34 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 6
  %35 = load i64, i64* %34, align 16
  %36 = load i8*, i8** %4, align 8
  %37 = call i32 @GET_UINT32_LE(i64 %35, i8* %36, i32 24)
  %38 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 7
  %39 = load i64, i64* %38, align 8
  %40 = load i8*, i8** %4, align 8
  %41 = call i32 @GET_UINT32_LE(i64 %39, i8* %40, i32 28)
  %42 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 8
  %43 = load i64, i64* %42, align 16
  %44 = load i8*, i8** %4, align 8
  %45 = call i32 @GET_UINT32_LE(i64 %43, i8* %44, i32 32)
  %46 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 9
  %47 = load i64, i64* %46, align 8
  %48 = load i8*, i8** %4, align 8
  %49 = call i32 @GET_UINT32_LE(i64 %47, i8* %48, i32 36)
  %50 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 10
  %51 = load i64, i64* %50, align 16
  %52 = load i8*, i8** %4, align 8
  %53 = call i32 @GET_UINT32_LE(i64 %51, i8* %52, i32 40)
  %54 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 11
  %55 = load i64, i64* %54, align 8
  %56 = load i8*, i8** %4, align 8
  %57 = call i32 @GET_UINT32_LE(i64 %55, i8* %56, i32 44)
  %58 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 12
  %59 = load i64, i64* %58, align 16
  %60 = load i8*, i8** %4, align 8
  %61 = call i32 @GET_UINT32_LE(i64 %59, i8* %60, i32 48)
  %62 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 13
  %63 = load i64, i64* %62, align 8
  %64 = load i8*, i8** %4, align 8
  %65 = call i32 @GET_UINT32_LE(i64 %63, i8* %64, i32 52)
  %66 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 14
  %67 = load i64, i64* %66, align 16
  %68 = load i8*, i8** %4, align 8
  %69 = call i32 @GET_UINT32_LE(i64 %67, i8* %68, i32 56)
  %70 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 15
  %71 = load i64, i64* %70, align 8
  %72 = load i8*, i8** %4, align 8
  %73 = call i32 @GET_UINT32_LE(i64 %71, i8* %72, i32 60)
  %74 = load %struct.TYPE_3__*, %struct.TYPE_3__** %3, align 8
  %75 = getelementptr inbounds %struct.TYPE_3__, %struct.TYPE_3__* %74, i32 0, i32 0
  %76 = load i64*, i64** %75, align 8
  %77 = getelementptr inbounds i64, i64* %76, i64 0
  %78 = load i64, i64* %77, align 8
  store i64 %78, i64* %6, align 8
  %79 = load %struct.TYPE_3__*, %struct.TYPE_3__** %3, align 8
  %80 = getelementptr inbounds %struct.TYPE_3__, %struct.TYPE_3__* %79, i32 0, i32 0
  %81 = load i64*, i64** %80, align 8
  %82 = getelementptr inbounds i64, i64* %81, i64 1
  %83 = load i64, i64* %82, align 8
  store i64 %83, i64* %7, align 8
  %84 = load %struct.TYPE_3__*, %struct.TYPE_3__** %3, align 8
  %85 = getelementptr inbounds %struct.TYPE_3__, %struct.TYPE_3__* %84, i32 0, i32 0
  %86 = load i64*, i64** %85, align 8
  %87 = getelementptr inbounds i64, i64* %86, i64 2
  %88 = load i64, i64* %87, align 8
  store i64 %88, i64* %8, align 8
  %89 = load %struct.TYPE_3__*, %struct.TYPE_3__** %3, align 8
  %90 = getelementptr inbounds %struct.TYPE_3__, %struct.TYPE_3__* %89, i32 0, i32 0
  %91 = load i64*, i64** %90, align 8
  %92 = getelementptr inbounds i64, i64* %91, i64 3
  %93 = load i64, i64* %92, align 8
  store i64 %93, i64* %9, align 8
  %94 = load i64, i64* %9, align 8
  %95 = load i64, i64* %7, align 8
  %96 = load i64, i64* %8, align 8
  %97 = load i64, i64* %9, align 8
  %98 = xor i64 %96, %97
  %99 = and i64 %95, %98
  %100 = xor i64 %94, %99
  %101 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 0
  %102 = load i64, i64* %101, align 16
  %103 = add nsw i64 %100, %102
  %104 = add nsw i64 %103, 3614090360
  %105 = load i64, i64* %6, align 8
  %106 = add nsw i64 %105, %104
  store i64 %106, i64* %6, align 8
  %107 = load i64, i64* %6, align 8
  %108 = shl i64 %107, 7
  %109 = load i64, i64* %6, align 8
  %110 = and i64 %109, 4294967295
  %111 = ashr i64 %110, 25
  %112 = or i64 %108, %111
  %113 = load i64, i64* %7, align 8
  %114 = add nsw i64 %112, %113
  store i64 %114, i64* %6, align 8
  %115 = load i64, i64* %8, align 8
  %116 = load i64, i64* %6, align 8
  %117 = load i64, i64* %7, align 8
  %118 = load i64, i64* %8, align 8
  %119 = xor i64 %117, %118
  %120 = and i64 %116, %119
  %121 = xor i64 %115, %120
  %122 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 1
  %123 = load i64, i64* %122, align 8
  %124 = add nsw i64 %121, %123
  %125 = add nsw i64 %124, 3905402710
  %126 = load i64, i64* %9, align 8
  %127 = add nsw i64 %126, %125
  store i64 %127, i64* %9, align 8
  %128 = load i64, i64* %9, align 8
  %129 = shl i64 %128, 12
  %130 = load i64, i64* %9, align 8
  %131 = and i64 %130, 4294967295
  %132 = ashr i64 %131, 20
  %133 = or i64 %129, %132
  %134 = load i64, i64* %6, align 8
  %135 = add nsw i64 %133, %134
  store i64 %135, i64* %9, align 8
  %136 = load i64, i64* %7, align 8
  %137 = load i64, i64* %9, align 8
  %138 = load i64, i64* %6, align 8
  %139 = load i64, i64* %7, align 8
  %140 = xor i64 %138, %139
  %141 = and i64 %137, %140
  %142 = xor i64 %136, %141
  %143 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 2
  %144 = load i64, i64* %143, align 16
  %145 = add nsw i64 %142, %144
  %146 = add nsw i64 %145, 606105819
  %147 = load i64, i64* %8, align 8
  %148 = add nsw i64 %147, %146
  store i64 %148, i64* %8, align 8
  %149 = load i64, i64* %8, align 8
  %150 = shl i64 %149, 17
  %151 = load i64, i64* %8, align 8
  %152 = and i64 %151, 4294967295
  %153 = ashr i64 %152, 15
  %154 = or i64 %150, %153
  %155 = load i64, i64* %9, align 8
  %156 = add nsw i64 %154, %155
  store i64 %156, i64* %8, align 8
  %157 = load i64, i64* %6, align 8
  %158 = load i64, i64* %8, align 8
  %159 = load i64, i64* %9, align 8
  %160 = load i64, i64* %6, align 8
  %161 = xor i64 %159, %160
  %162 = and i64 %158, %161
  %163 = xor i64 %157, %162
  %164 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 3
  %165 = load i64, i64* %164, align 8
  %166 = add nsw i64 %163, %165
  %167 = add nsw i64 %166, 3250441966
  %168 = load i64, i64* %7, align 8
  %169 = add nsw i64 %168, %167
  store i64 %169, i64* %7, align 8
  %170 = load i64, i64* %7, align 8
  %171 = shl i64 %170, 22
  %172 = load i64, i64* %7, align 8
  %173 = and i64 %172, 4294967295
  %174 = ashr i64 %173, 10
  %175 = or i64 %171, %174
  %176 = load i64, i64* %8, align 8
  %177 = add nsw i64 %175, %176
  store i64 %177, i64* %7, align 8
  %178 = load i64, i64* %9, align 8
  %179 = load i64, i64* %7, align 8
  %180 = load i64, i64* %8, align 8
  %181 = load i64, i64* %9, align 8
  %182 = xor i64 %180, %181
  %183 = and i64 %179, %182
  %184 = xor i64 %178, %183
  %185 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 4
  %186 = load i64, i64* %185, align 16
  %187 = add nsw i64 %184, %186
  %188 = add nsw i64 %187, 4118548399
  %189 = load i64, i64* %6, align 8
  %190 = add nsw i64 %189, %188
  store i64 %190, i64* %6, align 8
  %191 = load i64, i64* %6, align 8
  %192 = shl i64 %191, 7
  %193 = load i64, i64* %6, align 8
  %194 = and i64 %193, 4294967295
  %195 = ashr i64 %194, 25
  %196 = or i64 %192, %195
  %197 = load i64, i64* %7, align 8
  %198 = add nsw i64 %196, %197
  store i64 %198, i64* %6, align 8
  %199 = load i64, i64* %8, align 8
  %200 = load i64, i64* %6, align 8
  %201 = load i64, i64* %7, align 8
  %202 = load i64, i64* %8, align 8
  %203 = xor i64 %201, %202
  %204 = and i64 %200, %203
  %205 = xor i64 %199, %204
  %206 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 5
  %207 = load i64, i64* %206, align 8
  %208 = add nsw i64 %205, %207
  %209 = add nsw i64 %208, 1200080426
  %210 = load i64, i64* %9, align 8
  %211 = add nsw i64 %210, %209
  store i64 %211, i64* %9, align 8
  %212 = load i64, i64* %9, align 8
  %213 = shl i64 %212, 12
  %214 = load i64, i64* %9, align 8
  %215 = and i64 %214, 4294967295
  %216 = ashr i64 %215, 20
  %217 = or i64 %213, %216
  %218 = load i64, i64* %6, align 8
  %219 = add nsw i64 %217, %218
  store i64 %219, i64* %9, align 8
  %220 = load i64, i64* %7, align 8
  %221 = load i64, i64* %9, align 8
  %222 = load i64, i64* %6, align 8
  %223 = load i64, i64* %7, align 8
  %224 = xor i64 %222, %223
  %225 = and i64 %221, %224
  %226 = xor i64 %220, %225
  %227 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 6
  %228 = load i64, i64* %227, align 16
  %229 = add nsw i64 %226, %228
  %230 = add nsw i64 %229, 2821735955
  %231 = load i64, i64* %8, align 8
  %232 = add nsw i64 %231, %230
  store i64 %232, i64* %8, align 8
  %233 = load i64, i64* %8, align 8
  %234 = shl i64 %233, 17
  %235 = load i64, i64* %8, align 8
  %236 = and i64 %235, 4294967295
  %237 = ashr i64 %236, 15
  %238 = or i64 %234, %237
  %239 = load i64, i64* %9, align 8
  %240 = add nsw i64 %238, %239
  store i64 %240, i64* %8, align 8
  %241 = load i64, i64* %6, align 8
  %242 = load i64, i64* %8, align 8
  %243 = load i64, i64* %9, align 8
  %244 = load i64, i64* %6, align 8
  %245 = xor i64 %243, %244
  %246 = and i64 %242, %245
  %247 = xor i64 %241, %246
  %248 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 7
  %249 = load i64, i64* %248, align 8
  %250 = add nsw i64 %247, %249
  %251 = add nsw i64 %250, 4249261313
  %252 = load i64, i64* %7, align 8
  %253 = add nsw i64 %252, %251
  store i64 %253, i64* %7, align 8
  %254 = load i64, i64* %7, align 8
  %255 = shl i64 %254, 22
  %256 = load i64, i64* %7, align 8
  %257 = and i64 %256, 4294967295
  %258 = ashr i64 %257, 10
  %259 = or i64 %255, %258
  %260 = load i64, i64* %8, align 8
  %261 = add nsw i64 %259, %260
  store i64 %261, i64* %7, align 8
  %262 = load i64, i64* %9, align 8
  %263 = load i64, i64* %7, align 8
  %264 = load i64, i64* %8, align 8
  %265 = load i64, i64* %9, align 8
  %266 = xor i64 %264, %265
  %267 = and i64 %263, %266
  %268 = xor i64 %262, %267
  %269 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 8
  %270 = load i64, i64* %269, align 16
  %271 = add nsw i64 %268, %270
  %272 = add nsw i64 %271, 1770035416
  %273 = load i64, i64* %6, align 8
  %274 = add nsw i64 %273, %272
  store i64 %274, i64* %6, align 8
  %275 = load i64, i64* %6, align 8
  %276 = shl i64 %275, 7
  %277 = load i64, i64* %6, align 8
  %278 = and i64 %277, 4294967295
  %279 = ashr i64 %278, 25
  %280 = or i64 %276, %279
  %281 = load i64, i64* %7, align 8
  %282 = add nsw i64 %280, %281
  store i64 %282, i64* %6, align 8
  %283 = load i64, i64* %8, align 8
  %284 = load i64, i64* %6, align 8
  %285 = load i64, i64* %7, align 8
  %286 = load i64, i64* %8, align 8
  %287 = xor i64 %285, %286
  %288 = and i64 %284, %287
  %289 = xor i64 %283, %288
  %290 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 9
  %291 = load i64, i64* %290, align 8
  %292 = add nsw i64 %289, %291
  %293 = add nsw i64 %292, 2336552879
  %294 = load i64, i64* %9, align 8
  %295 = add nsw i64 %294, %293
  store i64 %295, i64* %9, align 8
  %296 = load i64, i64* %9, align 8
  %297 = shl i64 %296, 12
  %298 = load i64, i64* %9, align 8
  %299 = and i64 %298, 4294967295
  %300 = ashr i64 %299, 20
  %301 = or i64 %297, %300
  %302 = load i64, i64* %6, align 8
  %303 = add nsw i64 %301, %302
  store i64 %303, i64* %9, align 8
  %304 = load i64, i64* %7, align 8
  %305 = load i64, i64* %9, align 8
  %306 = load i64, i64* %6, align 8
  %307 = load i64, i64* %7, align 8
  %308 = xor i64 %306, %307
  %309 = and i64 %305, %308
  %310 = xor i64 %304, %309
  %311 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 10
  %312 = load i64, i64* %311, align 16
  %313 = add nsw i64 %310, %312
  %314 = add nsw i64 %313, 4294925233
  %315 = load i64, i64* %8, align 8
  %316 = add nsw i64 %315, %314
  store i64 %316, i64* %8, align 8
  %317 = load i64, i64* %8, align 8
  %318 = shl i64 %317, 17
  %319 = load i64, i64* %8, align 8
  %320 = and i64 %319, 4294967295
  %321 = ashr i64 %320, 15
  %322 = or i64 %318, %321
  %323 = load i64, i64* %9, align 8
  %324 = add nsw i64 %322, %323
  store i64 %324, i64* %8, align 8
  %325 = load i64, i64* %6, align 8
  %326 = load i64, i64* %8, align 8
  %327 = load i64, i64* %9, align 8
  %328 = load i64, i64* %6, align 8
  %329 = xor i64 %327, %328
  %330 = and i64 %326, %329
  %331 = xor i64 %325, %330
  %332 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 11
  %333 = load i64, i64* %332, align 8
  %334 = add nsw i64 %331, %333
  %335 = add nsw i64 %334, 2304563134
  %336 = load i64, i64* %7, align 8
  %337 = add nsw i64 %336, %335
  store i64 %337, i64* %7, align 8
  %338 = load i64, i64* %7, align 8
  %339 = shl i64 %338, 22
  %340 = load i64, i64* %7, align 8
  %341 = and i64 %340, 4294967295
  %342 = ashr i64 %341, 10
  %343 = or i64 %339, %342
  %344 = load i64, i64* %8, align 8
  %345 = add nsw i64 %343, %344
  store i64 %345, i64* %7, align 8
  %346 = load i64, i64* %9, align 8
  %347 = load i64, i64* %7, align 8
  %348 = load i64, i64* %8, align 8
  %349 = load i64, i64* %9, align 8
  %350 = xor i64 %348, %349
  %351 = and i64 %347, %350
  %352 = xor i64 %346, %351
  %353 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 12
  %354 = load i64, i64* %353, align 16
  %355 = add nsw i64 %352, %354
  %356 = add nsw i64 %355, 1804603682
  %357 = load i64, i64* %6, align 8
  %358 = add nsw i64 %357, %356
  store i64 %358, i64* %6, align 8
  %359 = load i64, i64* %6, align 8
  %360 = shl i64 %359, 7
  %361 = load i64, i64* %6, align 8
  %362 = and i64 %361, 4294967295
  %363 = ashr i64 %362, 25
  %364 = or i64 %360, %363
  %365 = load i64, i64* %7, align 8
  %366 = add nsw i64 %364, %365
  store i64 %366, i64* %6, align 8
  %367 = load i64, i64* %8, align 8
  %368 = load i64, i64* %6, align 8
  %369 = load i64, i64* %7, align 8
  %370 = load i64, i64* %8, align 8
  %371 = xor i64 %369, %370
  %372 = and i64 %368, %371
  %373 = xor i64 %367, %372
  %374 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 13
  %375 = load i64, i64* %374, align 8
  %376 = add nsw i64 %373, %375
  %377 = add nsw i64 %376, 4254626195
  %378 = load i64, i64* %9, align 8
  %379 = add nsw i64 %378, %377
  store i64 %379, i64* %9, align 8
  %380 = load i64, i64* %9, align 8
  %381 = shl i64 %380, 12
  %382 = load i64, i64* %9, align 8
  %383 = and i64 %382, 4294967295
  %384 = ashr i64 %383, 20
  %385 = or i64 %381, %384
  %386 = load i64, i64* %6, align 8
  %387 = add nsw i64 %385, %386
  store i64 %387, i64* %9, align 8
  %388 = load i64, i64* %7, align 8
  %389 = load i64, i64* %9, align 8
  %390 = load i64, i64* %6, align 8
  %391 = load i64, i64* %7, align 8
  %392 = xor i64 %390, %391
  %393 = and i64 %389, %392
  %394 = xor i64 %388, %393
  %395 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 14
  %396 = load i64, i64* %395, align 16
  %397 = add nsw i64 %394, %396
  %398 = add nsw i64 %397, 2792965006
  %399 = load i64, i64* %8, align 8
  %400 = add nsw i64 %399, %398
  store i64 %400, i64* %8, align 8
  %401 = load i64, i64* %8, align 8
  %402 = shl i64 %401, 17
  %403 = load i64, i64* %8, align 8
  %404 = and i64 %403, 4294967295
  %405 = ashr i64 %404, 15
  %406 = or i64 %402, %405
  %407 = load i64, i64* %9, align 8
  %408 = add nsw i64 %406, %407
  store i64 %408, i64* %8, align 8
  %409 = load i64, i64* %6, align 8
  %410 = load i64, i64* %8, align 8
  %411 = load i64, i64* %9, align 8
  %412 = load i64, i64* %6, align 8
  %413 = xor i64 %411, %412
  %414 = and i64 %410, %413
  %415 = xor i64 %409, %414
  %416 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 15
  %417 = load i64, i64* %416, align 8
  %418 = add nsw i64 %415, %417
  %419 = add nsw i64 %418, 1236535329
  %420 = load i64, i64* %7, align 8
  %421 = add nsw i64 %420, %419
  store i64 %421, i64* %7, align 8
  %422 = load i64, i64* %7, align 8
  %423 = shl i64 %422, 22
  %424 = load i64, i64* %7, align 8
  %425 = and i64 %424, 4294967295
  %426 = ashr i64 %425, 10
  %427 = or i64 %423, %426
  %428 = load i64, i64* %8, align 8
  %429 = add nsw i64 %427, %428
  store i64 %429, i64* %7, align 8
  %430 = load i64, i64* %8, align 8
  %431 = load i64, i64* %9, align 8
  %432 = load i64, i64* %7, align 8
  %433 = load i64, i64* %8, align 8
  %434 = xor i64 %432, %433
  %435 = and i64 %431, %434
  %436 = xor i64 %430, %435
  %437 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 1
  %438 = load i64, i64* %437, align 8
  %439 = add nsw i64 %436, %438
  %440 = add nsw i64 %439, 4129170786
  %441 = load i64, i64* %6, align 8
  %442 = add nsw i64 %441, %440
  store i64 %442, i64* %6, align 8
  %443 = load i64, i64* %6, align 8
  %444 = shl i64 %443, 5
  %445 = load i64, i64* %6, align 8
  %446 = and i64 %445, 4294967295
  %447 = ashr i64 %446, 27
  %448 = or i64 %444, %447
  %449 = load i64, i64* %7, align 8
  %450 = add nsw i64 %448, %449
  store i64 %450, i64* %6, align 8
  %451 = load i64, i64* %7, align 8
  %452 = load i64, i64* %8, align 8
  %453 = load i64, i64* %6, align 8
  %454 = load i64, i64* %7, align 8
  %455 = xor i64 %453, %454
  %456 = and i64 %452, %455
  %457 = xor i64 %451, %456
  %458 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 6
  %459 = load i64, i64* %458, align 16
  %460 = add nsw i64 %457, %459
  %461 = add nsw i64 %460, 3225465664
  %462 = load i64, i64* %9, align 8
  %463 = add nsw i64 %462, %461
  store i64 %463, i64* %9, align 8
  %464 = load i64, i64* %9, align 8
  %465 = shl i64 %464, 9
  %466 = load i64, i64* %9, align 8
  %467 = and i64 %466, 4294967295
  %468 = ashr i64 %467, 23
  %469 = or i64 %465, %468
  %470 = load i64, i64* %6, align 8
  %471 = add nsw i64 %469, %470
  store i64 %471, i64* %9, align 8
  %472 = load i64, i64* %6, align 8
  %473 = load i64, i64* %7, align 8
  %474 = load i64, i64* %9, align 8
  %475 = load i64, i64* %6, align 8
  %476 = xor i64 %474, %475
  %477 = and i64 %473, %476
  %478 = xor i64 %472, %477
  %479 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 11
  %480 = load i64, i64* %479, align 8
  %481 = add nsw i64 %478, %480
  %482 = add nsw i64 %481, 643717713
  %483 = load i64, i64* %8, align 8
  %484 = add nsw i64 %483, %482
  store i64 %484, i64* %8, align 8
  %485 = load i64, i64* %8, align 8
  %486 = shl i64 %485, 14
  %487 = load i64, i64* %8, align 8
  %488 = and i64 %487, 4294967295
  %489 = ashr i64 %488, 18
  %490 = or i64 %486, %489
  %491 = load i64, i64* %9, align 8
  %492 = add nsw i64 %490, %491
  store i64 %492, i64* %8, align 8
  %493 = load i64, i64* %9, align 8
  %494 = load i64, i64* %6, align 8
  %495 = load i64, i64* %8, align 8
  %496 = load i64, i64* %9, align 8
  %497 = xor i64 %495, %496
  %498 = and i64 %494, %497
  %499 = xor i64 %493, %498
  %500 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 0
  %501 = load i64, i64* %500, align 16
  %502 = add nsw i64 %499, %501
  %503 = add nsw i64 %502, 3921069994
  %504 = load i64, i64* %7, align 8
  %505 = add nsw i64 %504, %503
  store i64 %505, i64* %7, align 8
  %506 = load i64, i64* %7, align 8
  %507 = shl i64 %506, 20
  %508 = load i64, i64* %7, align 8
  %509 = and i64 %508, 4294967295
  %510 = ashr i64 %509, 12
  %511 = or i64 %507, %510
  %512 = load i64, i64* %8, align 8
  %513 = add nsw i64 %511, %512
  store i64 %513, i64* %7, align 8
  %514 = load i64, i64* %8, align 8
  %515 = load i64, i64* %9, align 8
  %516 = load i64, i64* %7, align 8
  %517 = load i64, i64* %8, align 8
  %518 = xor i64 %516, %517
  %519 = and i64 %515, %518
  %520 = xor i64 %514, %519
  %521 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 5
  %522 = load i64, i64* %521, align 8
  %523 = add nsw i64 %520, %522
  %524 = add nsw i64 %523, 3593408605
  %525 = load i64, i64* %6, align 8
  %526 = add nsw i64 %525, %524
  store i64 %526, i64* %6, align 8
  %527 = load i64, i64* %6, align 8
  %528 = shl i64 %527, 5
  %529 = load i64, i64* %6, align 8
  %530 = and i64 %529, 4294967295
  %531 = ashr i64 %530, 27
  %532 = or i64 %528, %531
  %533 = load i64, i64* %7, align 8
  %534 = add nsw i64 %532, %533
  store i64 %534, i64* %6, align 8
  %535 = load i64, i64* %7, align 8
  %536 = load i64, i64* %8, align 8
  %537 = load i64, i64* %6, align 8
  %538 = load i64, i64* %7, align 8
  %539 = xor i64 %537, %538
  %540 = and i64 %536, %539
  %541 = xor i64 %535, %540
  %542 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 10
  %543 = load i64, i64* %542, align 16
  %544 = add nsw i64 %541, %543
  %545 = add nsw i64 %544, 38016083
  %546 = load i64, i64* %9, align 8
  %547 = add nsw i64 %546, %545
  store i64 %547, i64* %9, align 8
  %548 = load i64, i64* %9, align 8
  %549 = shl i64 %548, 9
  %550 = load i64, i64* %9, align 8
  %551 = and i64 %550, 4294967295
  %552 = ashr i64 %551, 23
  %553 = or i64 %549, %552
  %554 = load i64, i64* %6, align 8
  %555 = add nsw i64 %553, %554
  store i64 %555, i64* %9, align 8
  %556 = load i64, i64* %6, align 8
  %557 = load i64, i64* %7, align 8
  %558 = load i64, i64* %9, align 8
  %559 = load i64, i64* %6, align 8
  %560 = xor i64 %558, %559
  %561 = and i64 %557, %560
  %562 = xor i64 %556, %561
  %563 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 15
  %564 = load i64, i64* %563, align 8
  %565 = add nsw i64 %562, %564
  %566 = add nsw i64 %565, 3634488961
  %567 = load i64, i64* %8, align 8
  %568 = add nsw i64 %567, %566
  store i64 %568, i64* %8, align 8
  %569 = load i64, i64* %8, align 8
  %570 = shl i64 %569, 14
  %571 = load i64, i64* %8, align 8
  %572 = and i64 %571, 4294967295
  %573 = ashr i64 %572, 18
  %574 = or i64 %570, %573
  %575 = load i64, i64* %9, align 8
  %576 = add nsw i64 %574, %575
  store i64 %576, i64* %8, align 8
  %577 = load i64, i64* %9, align 8
  %578 = load i64, i64* %6, align 8
  %579 = load i64, i64* %8, align 8
  %580 = load i64, i64* %9, align 8
  %581 = xor i64 %579, %580
  %582 = and i64 %578, %581
  %583 = xor i64 %577, %582
  %584 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 4
  %585 = load i64, i64* %584, align 16
  %586 = add nsw i64 %583, %585
  %587 = add nsw i64 %586, 3889429448
  %588 = load i64, i64* %7, align 8
  %589 = add nsw i64 %588, %587
  store i64 %589, i64* %7, align 8
  %590 = load i64, i64* %7, align 8
  %591 = shl i64 %590, 20
  %592 = load i64, i64* %7, align 8
  %593 = and i64 %592, 4294967295
  %594 = ashr i64 %593, 12
  %595 = or i64 %591, %594
  %596 = load i64, i64* %8, align 8
  %597 = add nsw i64 %595, %596
  store i64 %597, i64* %7, align 8
  %598 = load i64, i64* %8, align 8
  %599 = load i64, i64* %9, align 8
  %600 = load i64, i64* %7, align 8
  %601 = load i64, i64* %8, align 8
  %602 = xor i64 %600, %601
  %603 = and i64 %599, %602
  %604 = xor i64 %598, %603
  %605 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 9
  %606 = load i64, i64* %605, align 8
  %607 = add nsw i64 %604, %606
  %608 = add nsw i64 %607, 568446438
  %609 = load i64, i64* %6, align 8
  %610 = add nsw i64 %609, %608
  store i64 %610, i64* %6, align 8
  %611 = load i64, i64* %6, align 8
  %612 = shl i64 %611, 5
  %613 = load i64, i64* %6, align 8
  %614 = and i64 %613, 4294967295
  %615 = ashr i64 %614, 27
  %616 = or i64 %612, %615
  %617 = load i64, i64* %7, align 8
  %618 = add nsw i64 %616, %617
  store i64 %618, i64* %6, align 8
  %619 = load i64, i64* %7, align 8
  %620 = load i64, i64* %8, align 8
  %621 = load i64, i64* %6, align 8
  %622 = load i64, i64* %7, align 8
  %623 = xor i64 %621, %622
  %624 = and i64 %620, %623
  %625 = xor i64 %619, %624
  %626 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 14
  %627 = load i64, i64* %626, align 16
  %628 = add nsw i64 %625, %627
  %629 = add nsw i64 %628, 3275163606
  %630 = load i64, i64* %9, align 8
  %631 = add nsw i64 %630, %629
  store i64 %631, i64* %9, align 8
  %632 = load i64, i64* %9, align 8
  %633 = shl i64 %632, 9
  %634 = load i64, i64* %9, align 8
  %635 = and i64 %634, 4294967295
  %636 = ashr i64 %635, 23
  %637 = or i64 %633, %636
  %638 = load i64, i64* %6, align 8
  %639 = add nsw i64 %637, %638
  store i64 %639, i64* %9, align 8
  %640 = load i64, i64* %6, align 8
  %641 = load i64, i64* %7, align 8
  %642 = load i64, i64* %9, align 8
  %643 = load i64, i64* %6, align 8
  %644 = xor i64 %642, %643
  %645 = and i64 %641, %644
  %646 = xor i64 %640, %645
  %647 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 3
  %648 = load i64, i64* %647, align 8
  %649 = add nsw i64 %646, %648
  %650 = add nsw i64 %649, 4107603335
  %651 = load i64, i64* %8, align 8
  %652 = add nsw i64 %651, %650
  store i64 %652, i64* %8, align 8
  %653 = load i64, i64* %8, align 8
  %654 = shl i64 %653, 14
  %655 = load i64, i64* %8, align 8
  %656 = and i64 %655, 4294967295
  %657 = ashr i64 %656, 18
  %658 = or i64 %654, %657
  %659 = load i64, i64* %9, align 8
  %660 = add nsw i64 %658, %659
  store i64 %660, i64* %8, align 8
  %661 = load i64, i64* %9, align 8
  %662 = load i64, i64* %6, align 8
  %663 = load i64, i64* %8, align 8
  %664 = load i64, i64* %9, align 8
  %665 = xor i64 %663, %664
  %666 = and i64 %662, %665
  %667 = xor i64 %661, %666
  %668 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 8
  %669 = load i64, i64* %668, align 16
  %670 = add nsw i64 %667, %669
  %671 = add nsw i64 %670, 1163531501
  %672 = load i64, i64* %7, align 8
  %673 = add nsw i64 %672, %671
  store i64 %673, i64* %7, align 8
  %674 = load i64, i64* %7, align 8
  %675 = shl i64 %674, 20
  %676 = load i64, i64* %7, align 8
  %677 = and i64 %676, 4294967295
  %678 = ashr i64 %677, 12
  %679 = or i64 %675, %678
  %680 = load i64, i64* %8, align 8
  %681 = add nsw i64 %679, %680
  store i64 %681, i64* %7, align 8
  %682 = load i64, i64* %8, align 8
  %683 = load i64, i64* %9, align 8
  %684 = load i64, i64* %7, align 8
  %685 = load i64, i64* %8, align 8
  %686 = xor i64 %684, %685
  %687 = and i64 %683, %686
  %688 = xor i64 %682, %687
  %689 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 13
  %690 = load i64, i64* %689, align 8
  %691 = add nsw i64 %688, %690
  %692 = add nsw i64 %691, 2850285829
  %693 = load i64, i64* %6, align 8
  %694 = add nsw i64 %693, %692
  store i64 %694, i64* %6, align 8
  %695 = load i64, i64* %6, align 8
  %696 = shl i64 %695, 5
  %697 = load i64, i64* %6, align 8
  %698 = and i64 %697, 4294967295
  %699 = ashr i64 %698, 27
  %700 = or i64 %696, %699
  %701 = load i64, i64* %7, align 8
  %702 = add nsw i64 %700, %701
  store i64 %702, i64* %6, align 8
  %703 = load i64, i64* %7, align 8
  %704 = load i64, i64* %8, align 8
  %705 = load i64, i64* %6, align 8
  %706 = load i64, i64* %7, align 8
  %707 = xor i64 %705, %706
  %708 = and i64 %704, %707
  %709 = xor i64 %703, %708
  %710 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 2
  %711 = load i64, i64* %710, align 16
  %712 = add nsw i64 %709, %711
  %713 = add nsw i64 %712, 4243563512
  %714 = load i64, i64* %9, align 8
  %715 = add nsw i64 %714, %713
  store i64 %715, i64* %9, align 8
  %716 = load i64, i64* %9, align 8
  %717 = shl i64 %716, 9
  %718 = load i64, i64* %9, align 8
  %719 = and i64 %718, 4294967295
  %720 = ashr i64 %719, 23
  %721 = or i64 %717, %720
  %722 = load i64, i64* %6, align 8
  %723 = add nsw i64 %721, %722
  store i64 %723, i64* %9, align 8
  %724 = load i64, i64* %6, align 8
  %725 = load i64, i64* %7, align 8
  %726 = load i64, i64* %9, align 8
  %727 = load i64, i64* %6, align 8
  %728 = xor i64 %726, %727
  %729 = and i64 %725, %728
  %730 = xor i64 %724, %729
  %731 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 7
  %732 = load i64, i64* %731, align 8
  %733 = add nsw i64 %730, %732
  %734 = add nsw i64 %733, 1735328473
  %735 = load i64, i64* %8, align 8
  %736 = add nsw i64 %735, %734
  store i64 %736, i64* %8, align 8
  %737 = load i64, i64* %8, align 8
  %738 = shl i64 %737, 14
  %739 = load i64, i64* %8, align 8
  %740 = and i64 %739, 4294967295
  %741 = ashr i64 %740, 18
  %742 = or i64 %738, %741
  %743 = load i64, i64* %9, align 8
  %744 = add nsw i64 %742, %743
  store i64 %744, i64* %8, align 8
  %745 = load i64, i64* %9, align 8
  %746 = load i64, i64* %6, align 8
  %747 = load i64, i64* %8, align 8
  %748 = load i64, i64* %9, align 8
  %749 = xor i64 %747, %748
  %750 = and i64 %746, %749
  %751 = xor i64 %745, %750
  %752 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 12
  %753 = load i64, i64* %752, align 16
  %754 = add nsw i64 %751, %753
  %755 = add nsw i64 %754, 2368359562
  %756 = load i64, i64* %7, align 8
  %757 = add nsw i64 %756, %755
  store i64 %757, i64* %7, align 8
  %758 = load i64, i64* %7, align 8
  %759 = shl i64 %758, 20
  %760 = load i64, i64* %7, align 8
  %761 = and i64 %760, 4294967295
  %762 = ashr i64 %761, 12
  %763 = or i64 %759, %762
  %764 = load i64, i64* %8, align 8
  %765 = add nsw i64 %763, %764
  store i64 %765, i64* %7, align 8
  %766 = load i64, i64* %7, align 8
  %767 = load i64, i64* %8, align 8
  %768 = xor i64 %766, %767
  %769 = load i64, i64* %9, align 8
  %770 = xor i64 %768, %769
  %771 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 5
  %772 = load i64, i64* %771, align 8
  %773 = add nsw i64 %770, %772
  %774 = add nsw i64 %773, 4294588738
  %775 = load i64, i64* %6, align 8
  %776 = add nsw i64 %775, %774
  store i64 %776, i64* %6, align 8
  %777 = load i64, i64* %6, align 8
  %778 = shl i64 %777, 4
  %779 = load i64, i64* %6, align 8
  %780 = and i64 %779, 4294967295
  %781 = ashr i64 %780, 28
  %782 = or i64 %778, %781
  %783 = load i64, i64* %7, align 8
  %784 = add nsw i64 %782, %783
  store i64 %784, i64* %6, align 8
  %785 = load i64, i64* %6, align 8
  %786 = load i64, i64* %7, align 8
  %787 = xor i64 %785, %786
  %788 = load i64, i64* %8, align 8
  %789 = xor i64 %787, %788
  %790 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 8
  %791 = load i64, i64* %790, align 16
  %792 = add nsw i64 %789, %791
  %793 = add nsw i64 %792, 2272392833
  %794 = load i64, i64* %9, align 8
  %795 = add nsw i64 %794, %793
  store i64 %795, i64* %9, align 8
  %796 = load i64, i64* %9, align 8
  %797 = shl i64 %796, 11
  %798 = load i64, i64* %9, align 8
  %799 = and i64 %798, 4294967295
  %800 = ashr i64 %799, 21
  %801 = or i64 %797, %800
  %802 = load i64, i64* %6, align 8
  %803 = add nsw i64 %801, %802
  store i64 %803, i64* %9, align 8
  %804 = load i64, i64* %9, align 8
  %805 = load i64, i64* %6, align 8
  %806 = xor i64 %804, %805
  %807 = load i64, i64* %7, align 8
  %808 = xor i64 %806, %807
  %809 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 11
  %810 = load i64, i64* %809, align 8
  %811 = add nsw i64 %808, %810
  %812 = add nsw i64 %811, 1839030562
  %813 = load i64, i64* %8, align 8
  %814 = add nsw i64 %813, %812
  store i64 %814, i64* %8, align 8
  %815 = load i64, i64* %8, align 8
  %816 = shl i64 %815, 16
  %817 = load i64, i64* %8, align 8
  %818 = and i64 %817, 4294967295
  %819 = ashr i64 %818, 16
  %820 = or i64 %816, %819
  %821 = load i64, i64* %9, align 8
  %822 = add nsw i64 %820, %821
  store i64 %822, i64* %8, align 8
  %823 = load i64, i64* %8, align 8
  %824 = load i64, i64* %9, align 8
  %825 = xor i64 %823, %824
  %826 = load i64, i64* %6, align 8
  %827 = xor i64 %825, %826
  %828 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 14
  %829 = load i64, i64* %828, align 16
  %830 = add nsw i64 %827, %829
  %831 = add nsw i64 %830, 4259657740
  %832 = load i64, i64* %7, align 8
  %833 = add nsw i64 %832, %831
  store i64 %833, i64* %7, align 8
  %834 = load i64, i64* %7, align 8
  %835 = shl i64 %834, 23
  %836 = load i64, i64* %7, align 8
  %837 = and i64 %836, 4294967295
  %838 = ashr i64 %837, 9
  %839 = or i64 %835, %838
  %840 = load i64, i64* %8, align 8
  %841 = add nsw i64 %839, %840
  store i64 %841, i64* %7, align 8
  %842 = load i64, i64* %7, align 8
  %843 = load i64, i64* %8, align 8
  %844 = xor i64 %842, %843
  %845 = load i64, i64* %9, align 8
  %846 = xor i64 %844, %845
  %847 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 1
  %848 = load i64, i64* %847, align 8
  %849 = add nsw i64 %846, %848
  %850 = add nsw i64 %849, 2763975236
  %851 = load i64, i64* %6, align 8
  %852 = add nsw i64 %851, %850
  store i64 %852, i64* %6, align 8
  %853 = load i64, i64* %6, align 8
  %854 = shl i64 %853, 4
  %855 = load i64, i64* %6, align 8
  %856 = and i64 %855, 4294967295
  %857 = ashr i64 %856, 28
  %858 = or i64 %854, %857
  %859 = load i64, i64* %7, align 8
  %860 = add nsw i64 %858, %859
  store i64 %860, i64* %6, align 8
  %861 = load i64, i64* %6, align 8
  %862 = load i64, i64* %7, align 8
  %863 = xor i64 %861, %862
  %864 = load i64, i64* %8, align 8
  %865 = xor i64 %863, %864
  %866 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 4
  %867 = load i64, i64* %866, align 16
  %868 = add nsw i64 %865, %867
  %869 = add nsw i64 %868, 1272893353
  %870 = load i64, i64* %9, align 8
  %871 = add nsw i64 %870, %869
  store i64 %871, i64* %9, align 8
  %872 = load i64, i64* %9, align 8
  %873 = shl i64 %872, 11
  %874 = load i64, i64* %9, align 8
  %875 = and i64 %874, 4294967295
  %876 = ashr i64 %875, 21
  %877 = or i64 %873, %876
  %878 = load i64, i64* %6, align 8
  %879 = add nsw i64 %877, %878
  store i64 %879, i64* %9, align 8
  %880 = load i64, i64* %9, align 8
  %881 = load i64, i64* %6, align 8
  %882 = xor i64 %880, %881
  %883 = load i64, i64* %7, align 8
  %884 = xor i64 %882, %883
  %885 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 7
  %886 = load i64, i64* %885, align 8
  %887 = add nsw i64 %884, %886
  %888 = add nsw i64 %887, 4139469664
  %889 = load i64, i64* %8, align 8
  %890 = add nsw i64 %889, %888
  store i64 %890, i64* %8, align 8
  %891 = load i64, i64* %8, align 8
  %892 = shl i64 %891, 16
  %893 = load i64, i64* %8, align 8
  %894 = and i64 %893, 4294967295
  %895 = ashr i64 %894, 16
  %896 = or i64 %892, %895
  %897 = load i64, i64* %9, align 8
  %898 = add nsw i64 %896, %897
  store i64 %898, i64* %8, align 8
  %899 = load i64, i64* %8, align 8
  %900 = load i64, i64* %9, align 8
  %901 = xor i64 %899, %900
  %902 = load i64, i64* %6, align 8
  %903 = xor i64 %901, %902
  %904 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 10
  %905 = load i64, i64* %904, align 16
  %906 = add nsw i64 %903, %905
  %907 = add nsw i64 %906, 3200236656
  %908 = load i64, i64* %7, align 8
  %909 = add nsw i64 %908, %907
  store i64 %909, i64* %7, align 8
  %910 = load i64, i64* %7, align 8
  %911 = shl i64 %910, 23
  %912 = load i64, i64* %7, align 8
  %913 = and i64 %912, 4294967295
  %914 = ashr i64 %913, 9
  %915 = or i64 %911, %914
  %916 = load i64, i64* %8, align 8
  %917 = add nsw i64 %915, %916
  store i64 %917, i64* %7, align 8
  %918 = load i64, i64* %7, align 8
  %919 = load i64, i64* %8, align 8
  %920 = xor i64 %918, %919
  %921 = load i64, i64* %9, align 8
  %922 = xor i64 %920, %921
  %923 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 13
  %924 = load i64, i64* %923, align 8
  %925 = add nsw i64 %922, %924
  %926 = add nsw i64 %925, 681279174
  %927 = load i64, i64* %6, align 8
  %928 = add nsw i64 %927, %926
  store i64 %928, i64* %6, align 8
  %929 = load i64, i64* %6, align 8
  %930 = shl i64 %929, 4
  %931 = load i64, i64* %6, align 8
  %932 = and i64 %931, 4294967295
  %933 = ashr i64 %932, 28
  %934 = or i64 %930, %933
  %935 = load i64, i64* %7, align 8
  %936 = add nsw i64 %934, %935
  store i64 %936, i64* %6, align 8
  %937 = load i64, i64* %6, align 8
  %938 = load i64, i64* %7, align 8
  %939 = xor i64 %937, %938
  %940 = load i64, i64* %8, align 8
  %941 = xor i64 %939, %940
  %942 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 0
  %943 = load i64, i64* %942, align 16
  %944 = add nsw i64 %941, %943
  %945 = add nsw i64 %944, 3936430074
  %946 = load i64, i64* %9, align 8
  %947 = add nsw i64 %946, %945
  store i64 %947, i64* %9, align 8
  %948 = load i64, i64* %9, align 8
  %949 = shl i64 %948, 11
  %950 = load i64, i64* %9, align 8
  %951 = and i64 %950, 4294967295
  %952 = ashr i64 %951, 21
  %953 = or i64 %949, %952
  %954 = load i64, i64* %6, align 8
  %955 = add nsw i64 %953, %954
  store i64 %955, i64* %9, align 8
  %956 = load i64, i64* %9, align 8
  %957 = load i64, i64* %6, align 8
  %958 = xor i64 %956, %957
  %959 = load i64, i64* %7, align 8
  %960 = xor i64 %958, %959
  %961 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 3
  %962 = load i64, i64* %961, align 8
  %963 = add nsw i64 %960, %962
  %964 = add nsw i64 %963, 3572445317
  %965 = load i64, i64* %8, align 8
  %966 = add nsw i64 %965, %964
  store i64 %966, i64* %8, align 8
  %967 = load i64, i64* %8, align 8
  %968 = shl i64 %967, 16
  %969 = load i64, i64* %8, align 8
  %970 = and i64 %969, 4294967295
  %971 = ashr i64 %970, 16
  %972 = or i64 %968, %971
  %973 = load i64, i64* %9, align 8
  %974 = add nsw i64 %972, %973
  store i64 %974, i64* %8, align 8
  %975 = load i64, i64* %8, align 8
  %976 = load i64, i64* %9, align 8
  %977 = xor i64 %975, %976
  %978 = load i64, i64* %6, align 8
  %979 = xor i64 %977, %978
  %980 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 6
  %981 = load i64, i64* %980, align 16
  %982 = add nsw i64 %979, %981
  %983 = add nsw i64 %982, 76029189
  %984 = load i64, i64* %7, align 8
  %985 = add nsw i64 %984, %983
  store i64 %985, i64* %7, align 8
  %986 = load i64, i64* %7, align 8
  %987 = shl i64 %986, 23
  %988 = load i64, i64* %7, align 8
  %989 = and i64 %988, 4294967295
  %990 = ashr i64 %989, 9
  %991 = or i64 %987, %990
  %992 = load i64, i64* %8, align 8
  %993 = add nsw i64 %991, %992
  store i64 %993, i64* %7, align 8
  %994 = load i64, i64* %7, align 8
  %995 = load i64, i64* %8, align 8
  %996 = xor i64 %994, %995
  %997 = load i64, i64* %9, align 8
  %998 = xor i64 %996, %997
  %999 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 9
  %1000 = load i64, i64* %999, align 8
  %1001 = add nsw i64 %998, %1000
  %1002 = add nsw i64 %1001, 3654602809
  %1003 = load i64, i64* %6, align 8
  %1004 = add nsw i64 %1003, %1002
  store i64 %1004, i64* %6, align 8
  %1005 = load i64, i64* %6, align 8
  %1006 = shl i64 %1005, 4
  %1007 = load i64, i64* %6, align 8
  %1008 = and i64 %1007, 4294967295
  %1009 = ashr i64 %1008, 28
  %1010 = or i64 %1006, %1009
  %1011 = load i64, i64* %7, align 8
  %1012 = add nsw i64 %1010, %1011
  store i64 %1012, i64* %6, align 8
  %1013 = load i64, i64* %6, align 8
  %1014 = load i64, i64* %7, align 8
  %1015 = xor i64 %1013, %1014
  %1016 = load i64, i64* %8, align 8
  %1017 = xor i64 %1015, %1016
  %1018 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 12
  %1019 = load i64, i64* %1018, align 16
  %1020 = add nsw i64 %1017, %1019
  %1021 = add nsw i64 %1020, 3873151461
  %1022 = load i64, i64* %9, align 8
  %1023 = add nsw i64 %1022, %1021
  store i64 %1023, i64* %9, align 8
  %1024 = load i64, i64* %9, align 8
  %1025 = shl i64 %1024, 11
  %1026 = load i64, i64* %9, align 8
  %1027 = and i64 %1026, 4294967295
  %1028 = ashr i64 %1027, 21
  %1029 = or i64 %1025, %1028
  %1030 = load i64, i64* %6, align 8
  %1031 = add nsw i64 %1029, %1030
  store i64 %1031, i64* %9, align 8
  %1032 = load i64, i64* %9, align 8
  %1033 = load i64, i64* %6, align 8
  %1034 = xor i64 %1032, %1033
  %1035 = load i64, i64* %7, align 8
  %1036 = xor i64 %1034, %1035
  %1037 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 15
  %1038 = load i64, i64* %1037, align 8
  %1039 = add nsw i64 %1036, %1038
  %1040 = add nsw i64 %1039, 530742520
  %1041 = load i64, i64* %8, align 8
  %1042 = add nsw i64 %1041, %1040
  store i64 %1042, i64* %8, align 8
  %1043 = load i64, i64* %8, align 8
  %1044 = shl i64 %1043, 16
  %1045 = load i64, i64* %8, align 8
  %1046 = and i64 %1045, 4294967295
  %1047 = ashr i64 %1046, 16
  %1048 = or i64 %1044, %1047
  %1049 = load i64, i64* %9, align 8
  %1050 = add nsw i64 %1048, %1049
  store i64 %1050, i64* %8, align 8
  %1051 = load i64, i64* %8, align 8
  %1052 = load i64, i64* %9, align 8
  %1053 = xor i64 %1051, %1052
  %1054 = load i64, i64* %6, align 8
  %1055 = xor i64 %1053, %1054
  %1056 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 2
  %1057 = load i64, i64* %1056, align 16
  %1058 = add nsw i64 %1055, %1057
  %1059 = add nsw i64 %1058, 3299628645
  %1060 = load i64, i64* %7, align 8
  %1061 = add nsw i64 %1060, %1059
  store i64 %1061, i64* %7, align 8
  %1062 = load i64, i64* %7, align 8
  %1063 = shl i64 %1062, 23
  %1064 = load i64, i64* %7, align 8
  %1065 = and i64 %1064, 4294967295
  %1066 = ashr i64 %1065, 9
  %1067 = or i64 %1063, %1066
  %1068 = load i64, i64* %8, align 8
  %1069 = add nsw i64 %1067, %1068
  store i64 %1069, i64* %7, align 8
  %1070 = load i64, i64* %8, align 8
  %1071 = load i64, i64* %7, align 8
  %1072 = load i64, i64* %9, align 8
  %1073 = xor i64 %1072, -1
  %1074 = or i64 %1071, %1073
  %1075 = xor i64 %1070, %1074
  %1076 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 0
  %1077 = load i64, i64* %1076, align 16
  %1078 = add nsw i64 %1075, %1077
  %1079 = add nsw i64 %1078, 4096336452
  %1080 = load i64, i64* %6, align 8
  %1081 = add nsw i64 %1080, %1079
  store i64 %1081, i64* %6, align 8
  %1082 = load i64, i64* %6, align 8
  %1083 = shl i64 %1082, 6
  %1084 = load i64, i64* %6, align 8
  %1085 = and i64 %1084, 4294967295
  %1086 = ashr i64 %1085, 26
  %1087 = or i64 %1083, %1086
  %1088 = load i64, i64* %7, align 8
  %1089 = add nsw i64 %1087, %1088
  store i64 %1089, i64* %6, align 8
  %1090 = load i64, i64* %7, align 8
  %1091 = load i64, i64* %6, align 8
  %1092 = load i64, i64* %8, align 8
  %1093 = xor i64 %1092, -1
  %1094 = or i64 %1091, %1093
  %1095 = xor i64 %1090, %1094
  %1096 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 7
  %1097 = load i64, i64* %1096, align 8
  %1098 = add nsw i64 %1095, %1097
  %1099 = add nsw i64 %1098, 1126891415
  %1100 = load i64, i64* %9, align 8
  %1101 = add nsw i64 %1100, %1099
  store i64 %1101, i64* %9, align 8
  %1102 = load i64, i64* %9, align 8
  %1103 = shl i64 %1102, 10
  %1104 = load i64, i64* %9, align 8
  %1105 = and i64 %1104, 4294967295
  %1106 = ashr i64 %1105, 22
  %1107 = or i64 %1103, %1106
  %1108 = load i64, i64* %6, align 8
  %1109 = add nsw i64 %1107, %1108
  store i64 %1109, i64* %9, align 8
  %1110 = load i64, i64* %6, align 8
  %1111 = load i64, i64* %9, align 8
  %1112 = load i64, i64* %7, align 8
  %1113 = xor i64 %1112, -1
  %1114 = or i64 %1111, %1113
  %1115 = xor i64 %1110, %1114
  %1116 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 14
  %1117 = load i64, i64* %1116, align 16
  %1118 = add nsw i64 %1115, %1117
  %1119 = add nsw i64 %1118, 2878612391
  %1120 = load i64, i64* %8, align 8
  %1121 = add nsw i64 %1120, %1119
  store i64 %1121, i64* %8, align 8
  %1122 = load i64, i64* %8, align 8
  %1123 = shl i64 %1122, 15
  %1124 = load i64, i64* %8, align 8
  %1125 = and i64 %1124, 4294967295
  %1126 = ashr i64 %1125, 17
  %1127 = or i64 %1123, %1126
  %1128 = load i64, i64* %9, align 8
  %1129 = add nsw i64 %1127, %1128
  store i64 %1129, i64* %8, align 8
  %1130 = load i64, i64* %9, align 8
  %1131 = load i64, i64* %8, align 8
  %1132 = load i64, i64* %6, align 8
  %1133 = xor i64 %1132, -1
  %1134 = or i64 %1131, %1133
  %1135 = xor i64 %1130, %1134
  %1136 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 5
  %1137 = load i64, i64* %1136, align 8
  %1138 = add nsw i64 %1135, %1137
  %1139 = add nsw i64 %1138, 4237533241
  %1140 = load i64, i64* %7, align 8
  %1141 = add nsw i64 %1140, %1139
  store i64 %1141, i64* %7, align 8
  %1142 = load i64, i64* %7, align 8
  %1143 = shl i64 %1142, 21
  %1144 = load i64, i64* %7, align 8
  %1145 = and i64 %1144, 4294967295
  %1146 = ashr i64 %1145, 11
  %1147 = or i64 %1143, %1146
  %1148 = load i64, i64* %8, align 8
  %1149 = add nsw i64 %1147, %1148
  store i64 %1149, i64* %7, align 8
  %1150 = load i64, i64* %8, align 8
  %1151 = load i64, i64* %7, align 8
  %1152 = load i64, i64* %9, align 8
  %1153 = xor i64 %1152, -1
  %1154 = or i64 %1151, %1153
  %1155 = xor i64 %1150, %1154
  %1156 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 12
  %1157 = load i64, i64* %1156, align 16
  %1158 = add nsw i64 %1155, %1157
  %1159 = add nsw i64 %1158, 1700485571
  %1160 = load i64, i64* %6, align 8
  %1161 = add nsw i64 %1160, %1159
  store i64 %1161, i64* %6, align 8
  %1162 = load i64, i64* %6, align 8
  %1163 = shl i64 %1162, 6
  %1164 = load i64, i64* %6, align 8
  %1165 = and i64 %1164, 4294967295
  %1166 = ashr i64 %1165, 26
  %1167 = or i64 %1163, %1166
  %1168 = load i64, i64* %7, align 8
  %1169 = add nsw i64 %1167, %1168
  store i64 %1169, i64* %6, align 8
  %1170 = load i64, i64* %7, align 8
  %1171 = load i64, i64* %6, align 8
  %1172 = load i64, i64* %8, align 8
  %1173 = xor i64 %1172, -1
  %1174 = or i64 %1171, %1173
  %1175 = xor i64 %1170, %1174
  %1176 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 3
  %1177 = load i64, i64* %1176, align 8
  %1178 = add nsw i64 %1175, %1177
  %1179 = add nsw i64 %1178, 2399980690
  %1180 = load i64, i64* %9, align 8
  %1181 = add nsw i64 %1180, %1179
  store i64 %1181, i64* %9, align 8
  %1182 = load i64, i64* %9, align 8
  %1183 = shl i64 %1182, 10
  %1184 = load i64, i64* %9, align 8
  %1185 = and i64 %1184, 4294967295
  %1186 = ashr i64 %1185, 22
  %1187 = or i64 %1183, %1186
  %1188 = load i64, i64* %6, align 8
  %1189 = add nsw i64 %1187, %1188
  store i64 %1189, i64* %9, align 8
  %1190 = load i64, i64* %6, align 8
  %1191 = load i64, i64* %9, align 8
  %1192 = load i64, i64* %7, align 8
  %1193 = xor i64 %1192, -1
  %1194 = or i64 %1191, %1193
  %1195 = xor i64 %1190, %1194
  %1196 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 10
  %1197 = load i64, i64* %1196, align 16
  %1198 = add nsw i64 %1195, %1197
  %1199 = add nsw i64 %1198, 4293915773
  %1200 = load i64, i64* %8, align 8
  %1201 = add nsw i64 %1200, %1199
  store i64 %1201, i64* %8, align 8
  %1202 = load i64, i64* %8, align 8
  %1203 = shl i64 %1202, 15
  %1204 = load i64, i64* %8, align 8
  %1205 = and i64 %1204, 4294967295
  %1206 = ashr i64 %1205, 17
  %1207 = or i64 %1203, %1206
  %1208 = load i64, i64* %9, align 8
  %1209 = add nsw i64 %1207, %1208
  store i64 %1209, i64* %8, align 8
  %1210 = load i64, i64* %9, align 8
  %1211 = load i64, i64* %8, align 8
  %1212 = load i64, i64* %6, align 8
  %1213 = xor i64 %1212, -1
  %1214 = or i64 %1211, %1213
  %1215 = xor i64 %1210, %1214
  %1216 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 1
  %1217 = load i64, i64* %1216, align 8
  %1218 = add nsw i64 %1215, %1217
  %1219 = add nsw i64 %1218, 2240044497
  %1220 = load i64, i64* %7, align 8
  %1221 = add nsw i64 %1220, %1219
  store i64 %1221, i64* %7, align 8
  %1222 = load i64, i64* %7, align 8
  %1223 = shl i64 %1222, 21
  %1224 = load i64, i64* %7, align 8
  %1225 = and i64 %1224, 4294967295
  %1226 = ashr i64 %1225, 11
  %1227 = or i64 %1223, %1226
  %1228 = load i64, i64* %8, align 8
  %1229 = add nsw i64 %1227, %1228
  store i64 %1229, i64* %7, align 8
  %1230 = load i64, i64* %8, align 8
  %1231 = load i64, i64* %7, align 8
  %1232 = load i64, i64* %9, align 8
  %1233 = xor i64 %1232, -1
  %1234 = or i64 %1231, %1233
  %1235 = xor i64 %1230, %1234
  %1236 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 8
  %1237 = load i64, i64* %1236, align 16
  %1238 = add nsw i64 %1235, %1237
  %1239 = add nsw i64 %1238, 1873313359
  %1240 = load i64, i64* %6, align 8
  %1241 = add nsw i64 %1240, %1239
  store i64 %1241, i64* %6, align 8
  %1242 = load i64, i64* %6, align 8
  %1243 = shl i64 %1242, 6
  %1244 = load i64, i64* %6, align 8
  %1245 = and i64 %1244, 4294967295
  %1246 = ashr i64 %1245, 26
  %1247 = or i64 %1243, %1246
  %1248 = load i64, i64* %7, align 8
  %1249 = add nsw i64 %1247, %1248
  store i64 %1249, i64* %6, align 8
  %1250 = load i64, i64* %7, align 8
  %1251 = load i64, i64* %6, align 8
  %1252 = load i64, i64* %8, align 8
  %1253 = xor i64 %1252, -1
  %1254 = or i64 %1251, %1253
  %1255 = xor i64 %1250, %1254
  %1256 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 15
  %1257 = load i64, i64* %1256, align 8
  %1258 = add nsw i64 %1255, %1257
  %1259 = add nsw i64 %1258, 4264355552
  %1260 = load i64, i64* %9, align 8
  %1261 = add nsw i64 %1260, %1259
  store i64 %1261, i64* %9, align 8
  %1262 = load i64, i64* %9, align 8
  %1263 = shl i64 %1262, 10
  %1264 = load i64, i64* %9, align 8
  %1265 = and i64 %1264, 4294967295
  %1266 = ashr i64 %1265, 22
  %1267 = or i64 %1263, %1266
  %1268 = load i64, i64* %6, align 8
  %1269 = add nsw i64 %1267, %1268
  store i64 %1269, i64* %9, align 8
  %1270 = load i64, i64* %6, align 8
  %1271 = load i64, i64* %9, align 8
  %1272 = load i64, i64* %7, align 8
  %1273 = xor i64 %1272, -1
  %1274 = or i64 %1271, %1273
  %1275 = xor i64 %1270, %1274
  %1276 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 6
  %1277 = load i64, i64* %1276, align 16
  %1278 = add nsw i64 %1275, %1277
  %1279 = add nsw i64 %1278, 2734768916
  %1280 = load i64, i64* %8, align 8
  %1281 = add nsw i64 %1280, %1279
  store i64 %1281, i64* %8, align 8
  %1282 = load i64, i64* %8, align 8
  %1283 = shl i64 %1282, 15
  %1284 = load i64, i64* %8, align 8
  %1285 = and i64 %1284, 4294967295
  %1286 = ashr i64 %1285, 17
  %1287 = or i64 %1283, %1286
  %1288 = load i64, i64* %9, align 8
  %1289 = add nsw i64 %1287, %1288
  store i64 %1289, i64* %8, align 8
  %1290 = load i64, i64* %9, align 8
  %1291 = load i64, i64* %8, align 8
  %1292 = load i64, i64* %6, align 8
  %1293 = xor i64 %1292, -1
  %1294 = or i64 %1291, %1293
  %1295 = xor i64 %1290, %1294
  %1296 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 13
  %1297 = load i64, i64* %1296, align 8
  %1298 = add nsw i64 %1295, %1297
  %1299 = add nsw i64 %1298, 1309151649
  %1300 = load i64, i64* %7, align 8
  %1301 = add nsw i64 %1300, %1299
  store i64 %1301, i64* %7, align 8
  %1302 = load i64, i64* %7, align 8
  %1303 = shl i64 %1302, 21
  %1304 = load i64, i64* %7, align 8
  %1305 = and i64 %1304, 4294967295
  %1306 = ashr i64 %1305, 11
  %1307 = or i64 %1303, %1306
  %1308 = load i64, i64* %8, align 8
  %1309 = add nsw i64 %1307, %1308
  store i64 %1309, i64* %7, align 8
  %1310 = load i64, i64* %8, align 8
  %1311 = load i64, i64* %7, align 8
  %1312 = load i64, i64* %9, align 8
  %1313 = xor i64 %1312, -1
  %1314 = or i64 %1311, %1313
  %1315 = xor i64 %1310, %1314
  %1316 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 4
  %1317 = load i64, i64* %1316, align 16
  %1318 = add nsw i64 %1315, %1317
  %1319 = add nsw i64 %1318, 4149444226
  %1320 = load i64, i64* %6, align 8
  %1321 = add nsw i64 %1320, %1319
  store i64 %1321, i64* %6, align 8
  %1322 = load i64, i64* %6, align 8
  %1323 = shl i64 %1322, 6
  %1324 = load i64, i64* %6, align 8
  %1325 = and i64 %1324, 4294967295
  %1326 = ashr i64 %1325, 26
  %1327 = or i64 %1323, %1326
  %1328 = load i64, i64* %7, align 8
  %1329 = add nsw i64 %1327, %1328
  store i64 %1329, i64* %6, align 8
  %1330 = load i64, i64* %7, align 8
  %1331 = load i64, i64* %6, align 8
  %1332 = load i64, i64* %8, align 8
  %1333 = xor i64 %1332, -1
  %1334 = or i64 %1331, %1333
  %1335 = xor i64 %1330, %1334
  %1336 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 11
  %1337 = load i64, i64* %1336, align 8
  %1338 = add nsw i64 %1335, %1337
  %1339 = add nsw i64 %1338, 3174756917
  %1340 = load i64, i64* %9, align 8
  %1341 = add nsw i64 %1340, %1339
  store i64 %1341, i64* %9, align 8
  %1342 = load i64, i64* %9, align 8
  %1343 = shl i64 %1342, 10
  %1344 = load i64, i64* %9, align 8
  %1345 = and i64 %1344, 4294967295
  %1346 = ashr i64 %1345, 22
  %1347 = or i64 %1343, %1346
  %1348 = load i64, i64* %6, align 8
  %1349 = add nsw i64 %1347, %1348
  store i64 %1349, i64* %9, align 8
  %1350 = load i64, i64* %6, align 8
  %1351 = load i64, i64* %9, align 8
  %1352 = load i64, i64* %7, align 8
  %1353 = xor i64 %1352, -1
  %1354 = or i64 %1351, %1353
  %1355 = xor i64 %1350, %1354
  %1356 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 2
  %1357 = load i64, i64* %1356, align 16
  %1358 = add nsw i64 %1355, %1357
  %1359 = add nsw i64 %1358, 718787259
  %1360 = load i64, i64* %8, align 8
  %1361 = add nsw i64 %1360, %1359
  store i64 %1361, i64* %8, align 8
  %1362 = load i64, i64* %8, align 8
  %1363 = shl i64 %1362, 15
  %1364 = load i64, i64* %8, align 8
  %1365 = and i64 %1364, 4294967295
  %1366 = ashr i64 %1365, 17
  %1367 = or i64 %1363, %1366
  %1368 = load i64, i64* %9, align 8
  %1369 = add nsw i64 %1367, %1368
  store i64 %1369, i64* %8, align 8
  %1370 = load i64, i64* %9, align 8
  %1371 = load i64, i64* %8, align 8
  %1372 = load i64, i64* %6, align 8
  %1373 = xor i64 %1372, -1
  %1374 = or i64 %1371, %1373
  %1375 = xor i64 %1370, %1374
  %1376 = getelementptr inbounds [16 x i64], [16 x i64]* %5, i64 0, i64 9
  %1377 = load i64, i64* %1376, align 8
  %1378 = add nsw i64 %1375, %1377
  %1379 = add nsw i64 %1378, 3951481745
  %1380 = load i64, i64* %7, align 8
  %1381 = add nsw i64 %1380, %1379
  store i64 %1381, i64* %7, align 8
  %1382 = load i64, i64* %7, align 8
  %1383 = shl i64 %1382, 21
  %1384 = load i64, i64* %7, align 8
  %1385 = and i64 %1384, 4294967295
  %1386 = ashr i64 %1385, 11
  %1387 = or i64 %1383, %1386
  %1388 = load i64, i64* %8, align 8
  %1389 = add nsw i64 %1387, %1388
  store i64 %1389, i64* %7, align 8
  %1390 = load i64, i64* %6, align 8
  %1391 = load %struct.TYPE_3__*, %struct.TYPE_3__** %3, align 8
  %1392 = getelementptr inbounds %struct.TYPE_3__, %struct.TYPE_3__* %1391, i32 0, i32 0
  %1393 = load i64*, i64** %1392, align 8
  %1394 = getelementptr inbounds i64, i64* %1393, i64 0
  %1395 = load i64, i64* %1394, align 8
  %1396 = add nsw i64 %1395, %1390
  store i64 %1396, i64* %1394, align 8
  %1397 = load i64, i64* %7, align 8
  %1398 = load %struct.TYPE_3__*, %struct.TYPE_3__** %3, align 8
  %1399 = getelementptr inbounds %struct.TYPE_3__, %struct.TYPE_3__* %1398, i32 0, i32 0
  %1400 = load i64*, i64** %1399, align 8
  %1401 = getelementptr inbounds i64, i64* %1400, i64 1
  %1402 = load i64, i64* %1401, align 8
  %1403 = add nsw i64 %1402, %1397
  store i64 %1403, i64* %1401, align 8
  %1404 = load i64, i64* %8, align 8
  %1405 = load %struct.TYPE_3__*, %struct.TYPE_3__** %3, align 8
  %1406 = getelementptr inbounds %struct.TYPE_3__, %struct.TYPE_3__* %1405, i32 0, i32 0
  %1407 = load i64*, i64** %1406, align 8
  %1408 = getelementptr inbounds i64, i64* %1407, i64 2
  %1409 = load i64, i64* %1408, align 8
  %1410 = add nsw i64 %1409, %1404
  store i64 %1410, i64* %1408, align 8
  %1411 = load i64, i64* %9, align 8
  %1412 = load %struct.TYPE_3__*, %struct.TYPE_3__** %3, align 8
  %1413 = getelementptr inbounds %struct.TYPE_3__, %struct.TYPE_3__* %1412, i32 0, i32 0
  %1414 = load i64*, i64** %1413, align 8
  %1415 = getelementptr inbounds i64, i64* %1414, i64 3
  %1416 = load i64, i64* %1415, align 8
  %1417 = add nsw i64 %1416, %1411
  store i64 %1417, i64* %1415, align 8
  ret void
}

declare dso_local i32 @GET_UINT32_LE(i64, i8*, i32) #1

attributes #0 = { noinline nounwind optnone uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "frame-pointer"="all" "less-precise-fpmad"="false" "min-legal-vector-width"="0" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #1 = { "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "frame-pointer"="all" "less-precise-fpmad"="false" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }

!llvm.module.flags = !{!0}
!llvm.ident = !{!1}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{!"clang version 10.0.1 (https://github.com/wsmoses/llvm-project-tok c8e5003577614e72d6d18a216e6a09771e1fcce4)"}
